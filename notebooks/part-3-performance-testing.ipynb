{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv; load_dotenv()\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_models = {\n",
    "    50: 'ft:gpt-3.5-turbo-1106:aa-engineering::8I9g9RO0',\n",
    "    100: 'ft:gpt-3.5-turbo-1106:aa-engineering::8I9vALSP',\n",
    "    200: 'ft:gpt-3.5-turbo-1106:aa-engineering::8IAIy8LD'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Classification APIs\n",
    "\n",
    "These APIs take an input message and use either a fine-tuned model or the general-purpose model to predict whether it is spam.  Each returns a boolean: True for spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install retry\n",
    "from retry import retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tuned Model API\n",
    "\n",
    "fineTunePrompt = \"You are a system for categorizing SMS text messages as being unwanted spam or normal messages.\"\n",
    "\n",
    "@retry(delay=0, backoff=2, max_delay=10)\n",
    "async def getSpamClassification_FineTune(fineTunedModelId, prompt):\n",
    "  completion = await openai.ChatCompletion.acreate(\n",
    "    model=fineTunedModelId,\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": fineTunePrompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "  )\n",
    "  result = completion.choices[0].message.content.lower() == 'spam'\n",
    "  # print(prompt, \"=>\", result)\n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Purpose Model API\n",
    "\n",
    "generalModelPrompt = \"You will be provided with a text message. You will need to classify the text message as spam, ham. Spam is a text message that is spam, harmful, abusive, or otherwise unwanted. Ham is a text message that is not spam.\"\n",
    "\n",
    "@retry(delay=0, backoff=2, max_delay=10)\n",
    "async def getSpamClassification_GeneralModel(message):\n",
    "    response = await openai.ChatCompletion.acreate(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": generalModelPrompt},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=256\n",
    "    )\n",
    "    return response.choices[0].message.content.lower() == 'spam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Validation Data\n",
    "\n",
    "Each fine-tuned model has a validation dataset in addition to its training data.  Here we predict on those datasets for each fine tuned model and the general-purpose model.\n",
    "\n",
    "Predicting on the entire validation set takes some time.  OpenAI has a rate limit of 60 requests per minute.  It's also not free, so it's not something we want to have to do more than once.\n",
    "\n",
    "To make this code robust to things like network errors, we start by creating a dataframe that contains the validation data and a blank column for the results.  The code will run predictions for each row that has an empty result.  This means that this code can be restarted in case of failure.\n",
    "\n",
    "Additionally, to avoid having to re-run all the predictions in case of kernel restart, we save the resulting dataframe to file where it can be optionally reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared empty validation dataframe with 700 rows\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for sample_size in fine_tuned_models.keys():\n",
    "    fineTunedModelId = fine_tuned_models[sample_size]\n",
    "    validation_data_path = f\"../data/temp/model_{sample_size}/validation.jsonl\"\n",
    "    with open(validation_data_path, 'r') as f:\n",
    "\n",
    "        # To test this on a smaller dataset, we can optionally use \"[:5]\" to take only the first 5 lines\n",
    "        # for line in f.readlines()[:5]:\n",
    "        for line in f.readlines():\n",
    "            data = json.loads(line)\n",
    "            prompt = data['messages'][1]['content']\n",
    "            completion = data['messages'][2]['content']\n",
    "            rows.append({\n",
    "                'model': fineTunedModelId,\n",
    "                'sample_size': sample_size,\n",
    "                'prompt': prompt,\n",
    "                'expected': completion == 'spam',\n",
    "                'predicted': None\n",
    "            })\n",
    "            rows.append({\n",
    "                'model': 'general',\n",
    "                'sample_size': sample_size,\n",
    "                'prompt': prompt,\n",
    "                'expected': completion == 'spam',\n",
    "                'predicted': None\n",
    "            })    \n",
    "\n",
    "validation_df = pd.DataFrame(rows)      \n",
    "print(\"Prepared empty validation dataframe with {} rows\".format(len(validation_df)))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a previous result is available, we can optionally load it here instead of re-running the validation\n",
    "# validation_df = pd.read_csv('../data/temp/validation_results.csv')\n",
    "# validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation, 581 items remaining\n",
      "0:00:00.006995  Predicting on validation row 120 / 700\n",
      "0:00:00.413971  Predicting on validation row 121 / 700\n",
      "0:00:00.687551  Predicting on validation row 122 / 700\n",
      "0:00:01.249849  Predicting on validation row 123 / 700\n",
      "0:00:01.571406  Predicting on validation row 124 / 700\n",
      "0:00:01.945423  Predicting on validation row 125 / 700\n",
      "0:00:02.259776  Predicting on validation row 126 / 700\n",
      "0:00:02.651120  Predicting on validation row 127 / 700\n",
      "0:00:02.939800  Predicting on validation row 128 / 700\n",
      "0:00:03.337975  Predicting on validation row 129 / 700\n",
      "0:00:03.624033  Predicting on validation row 130 / 700\n",
      "0:00:04.029996  Predicting on validation row 131 / 700\n",
      "0:00:04.427712  Predicting on validation row 132 / 700\n",
      "0:00:05.213995  Predicting on validation row 133 / 700\n",
      "0:00:05.468360  Predicting on validation row 134 / 700\n",
      "0:00:07.072667  Predicting on validation row 135 / 700\n",
      "0:00:07.347170  Predicting on validation row 136 / 700\n",
      "0:00:07.923684  Predicting on validation row 137 / 700\n",
      "0:00:08.164990  Predicting on validation row 138 / 700\n",
      "0:00:08.532480  Predicting on validation row 139 / 700\n",
      "0:00:08.806191  Predicting on validation row 140 / 700\n",
      "0:00:09.408080  Predicting on validation row 141 / 700\n",
      "0:00:09.661840  Predicting on validation row 142 / 700\n",
      "0:00:10.028242  Predicting on validation row 143 / 700\n",
      "0:00:10.341307  Predicting on validation row 144 / 700\n",
      "0:00:11.871433  Predicting on validation row 145 / 700\n",
      "0:00:12.182742  Predicting on validation row 146 / 700\n",
      "0:00:12.834925  Predicting on validation row 147 / 700\n",
      "0:00:13.190508  Predicting on validation row 148 / 700\n",
      "0:00:13.604023  Predicting on validation row 149 / 700\n",
      "0:00:13.868806  Predicting on validation row 150 / 700\n",
      "0:00:14.448969  Predicting on validation row 151 / 700\n",
      "0:00:14.685480  Predicting on validation row 152 / 700\n",
      "0:00:15.267658  Predicting on validation row 153 / 700\n",
      "0:00:15.524930  Predicting on validation row 154 / 700\n",
      "0:00:15.872847  Predicting on validation row 155 / 700\n",
      "0:00:16.182729  Predicting on validation row 156 / 700\n",
      "0:00:16.580771  Predicting on validation row 157 / 700\n",
      "0:00:16.850835  Predicting on validation row 158 / 700\n",
      "0:00:17.467906  Predicting on validation row 159 / 700\n",
      "0:00:17.718067  Predicting on validation row 160 / 700\n",
      "0:00:18.290773  Predicting on validation row 161 / 700\n",
      "0:00:18.571156  Predicting on validation row 162 / 700\n",
      "0:00:19.132145  Predicting on validation row 163 / 700\n",
      "0:00:19.406984  Predicting on validation row 164 / 700\n",
      "0:00:19.871917  Predicting on validation row 165 / 700\n",
      "0:00:20.159943  Predicting on validation row 166 / 700\n",
      "0:00:20.565274  Predicting on validation row 167 / 700\n",
      "0:00:20.844723  Predicting on validation row 168 / 700\n",
      "0:00:21.363063  Predicting on validation row 169 / 700\n",
      "0:00:21.639095  Predicting on validation row 170 / 700\n",
      "0:00:22.254170  Predicting on validation row 171 / 700\n",
      "0:00:22.534449  Predicting on validation row 172 / 700\n",
      "0:00:23.106774  Predicting on validation row 173 / 700\n",
      "0:00:23.376364  Predicting on validation row 174 / 700\n",
      "0:00:23.928611  Predicting on validation row 175 / 700\n",
      "0:00:24.177631  Predicting on validation row 176 / 700\n",
      "0:00:24.718203  Predicting on validation row 177 / 700\n"
     ]
    }
   ],
   "source": [
    "# Note: Running this cell will take a while and incur API usage costs\n",
    "\n",
    "# Use Throttler to limit the number of requests per minute\n",
    "# %pip install throttler\n",
    "from throttler import Throttler\n",
    "throttler = Throttler(rate_limit=19, period=20)\n",
    "\n",
    "# Use Semaphore to control the number of concurrent requests\n",
    "import asyncio\n",
    "semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "print(\"Running validation, {} items remaining\".format(validation_df['predicted'].isnull().sum()))\n",
    "\n",
    "# async def classifyRow(index, row):\n",
    "#     async with semaphore:\n",
    "#         async with throttler:\n",
    "#             print(\"Predicting on validation row {} / {}\".format(index+1, len(validation_df)))\n",
    "#             if row['model'] == 'general':\n",
    "#                 result = await getSpamClassification_GeneralModel(row['prompt'])\n",
    "#             else:\n",
    "#                 result = await getSpamClassification_FineTune(row['model'], row['prompt'])\n",
    "#         validation_df.loc[index, 'predicted'] = result\n",
    "\n",
    "# tasks = [classifyRow(index, row) for index, row in validation_df.iterrows()]\n",
    "\n",
    "# # Schedule the tasks for execution\n",
    "# for task in tasks:\n",
    "#     asyncio.ensure_future(task)\n",
    "\n",
    "# # Wait for all tasks to complete\n",
    "# await asyncio.gather(*tasks)\n",
    "\n",
    "# asyncio.run(asyncio.gather(*(classifyRow(index, row) for index, row in validation_df.iterrows())))        \n",
    "\n",
    "start = time.time()\n",
    "for index, row in validation_df.iterrows():\n",
    "    if row['predicted'] is None:\n",
    "        async with throttler:\n",
    "            elapsedSeconds = time.time() - start\n",
    "            print(\"{}  Predicting on validation row {} / {}\".format(str(datetime.timedelta(seconds=elapsedSeconds)), index+1, len(validation_df)))\n",
    "            if row['model'] == 'general':\n",
    "                result = await getSpamClassification_GeneralModel(row['prompt'])\n",
    "            else:\n",
    "                result = await getSpamClassification_FineTune(row['model'], row['prompt'])\n",
    "        validation_df.loc[index, 'predicted'] = result\n",
    "\n",
    "validation_df['predicted'] = validation_df['predicted'].astype(bool)\n",
    "validation_df['correct'] = validation_df['expected'] == validation_df['predicted']\n",
    "validation_df.to_csv('../data/temp/validation_results.csv', index=False)\n",
    "print(\"Saved validation results to ../data/temp/validation_results.csv\")\n",
    "validation_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_size</th>\n",
       "      <th>model</th>\n",
       "      <th>true_positive</th>\n",
       "      <th>false_positive</th>\n",
       "      <th>false_negative</th>\n",
       "      <th>true_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>general</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>general</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>general</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_size       model  true_positive  false_positive  false_negative  \\\n",
       "0           50  fine-tuned              5               0               0   \n",
       "1           50     general              5               0               0   \n",
       "2          100  fine-tuned              5               0               0   \n",
       "3          100     general              5               0               0   \n",
       "4          200  fine-tuned              5               0               0   \n",
       "5          200     general              5               0               0   \n",
       "\n",
       "   true_negative  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "5              0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a confusion matrix for each sample size and model, put them into a dataframe\n",
    "\n",
    "#%pip install scikit-learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "grouped = validation_df.groupby('sample_size')\n",
    "\n",
    "rows = []\n",
    "for sample_size in grouped.groups:\n",
    "    group = grouped.get_group(sample_size)\n",
    "    # display(group)\n",
    "\n",
    "    fineTuneModelPredictions = group[group['model'] != 'general']\n",
    "    generalModelPredictions = group[group['model'] == 'general']\n",
    "\n",
    "    fineTuneConfusionMatrix = confusion_matrix(fineTuneModelPredictions['expected'], fineTuneModelPredictions['predicted'], labels=[True, False])\n",
    "    # print(fineTuneConfusionMatrix)\n",
    "    fineTuneModelAccuracy = (fineTuneConfusionMatrix[0][0] + fineTuneConfusionMatrix[1][1]) / (fineTuneConfusionMatrix[0][0] + fineTuneConfusionMatrix[0][1] + fineTuneConfusionMatrix[1][0] + fineTuneConfusionMatrix[1][1])\n",
    "    rows.append([sample_size, 'fine-tuned', fineTuneConfusionMatrix[0][0], fineTuneConfusionMatrix[0][1], fineTuneConfusionMatrix[1][0], fineTuneConfusionMatrix[1][1], fineTuneModelAccuracy])\n",
    "\n",
    "    generalConfusionMatrix = confusion_matrix(generalModelPredictions['expected'], generalModelPredictions['predicted'], labels=[True, False])\n",
    "    # print(generalConfusionMatrix)\n",
    "    generalModelAccuracy = (generalConfusionMatrix[0][0] + generalConfusionMatrix[1][1]) / (generalConfusionMatrix[0][0] + generalConfusionMatrix[0][1] + generalConfusionMatrix[1][0] + generalConfusionMatrix[1][1])\n",
    "    rows.append([sample_size, 'general', generalConfusionMatrix[0][0], generalConfusionMatrix[0][1], generalConfusionMatrix[1][0], generalConfusionMatrix[1][1], generalModelAccuracy])\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(rows, columns=['sample_size', 'model', 'true_positive', 'false_positive', 'false_negative', 'true_negative', 'accuracy'])\n",
    "confusion_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
