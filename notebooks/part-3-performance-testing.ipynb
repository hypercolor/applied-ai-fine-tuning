{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Performance Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Now that we can classify SMS messages using both the general-purpose GPT model and our fine tuned models, we want to test them to see if there are performance and cost differences.\n",
    "\n",
    "The following steps are covered:\n",
    "\n",
    "* Set up classification APIs for both fine-tuned and general-purpose models (re-used from Part 1 and Part 2)\n",
    "* Predict on the validation data for each model\n",
    "* Look at performance via confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# %pip install pandas\n",
    "# %pip install python-dotenv\n",
    "# %pip install --pre openai\n",
    "# %pip install throttler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "* https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv; load_dotenv()\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_models = {\n",
    "    50: 'ft:gpt-3.5-turbo-1106:aa-engineering::8I9g9RO0',\n",
    "    100: 'ft:gpt-3.5-turbo-1106:aa-engineering::8I9vALSP',\n",
    "    200: 'ft:gpt-3.5-turbo-1106:aa-engineering::8IAIy8LD'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Classification APIs\n",
    "\n",
    "These APIs take an input message and use either a fine-tuned model or the general-purpose model to predict whether it is spam.  Each returns a boolean: True for spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install retry\n",
    "from retry import retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tuned Model API\n",
    "\n",
    "fineTunePrompt = \"You are a system for categorizing SMS text messages as being unwanted spam or normal messages.\"\n",
    "\n",
    "@retry(delay=0, backoff=2, max_delay=10)\n",
    "async def getSpamClassification_FineTune(fineTunedModelId, prompt):\n",
    "  completion = await openai.ChatCompletion.acreate(\n",
    "    model=fineTunedModelId,\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": fineTunePrompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "  )\n",
    "  result = completion.choices[0].message.content.lower() == 'spam'\n",
    "  # print(prompt, \"=>\", result)\n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Purpose Model API\n",
    "\n",
    "generalModelPrompt = \"You will be provided with a text message. You will need to classify the text message as spam, ham. Spam is a text message that is spam, harmful, abusive, or otherwise unwanted. Ham is a text message that is not spam.\"\n",
    "\n",
    "@retry(delay=0, backoff=2, max_delay=10)\n",
    "async def getSpamClassification_GeneralModel(message):\n",
    "    response = await openai.ChatCompletion.acreate(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": generalModelPrompt},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=256\n",
    "    )\n",
    "    return response.choices[0].message.content.lower() == 'spam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Validation Data\n",
    "\n",
    "Each fine-tuned model has a validation dataset in addition to its training data.  Here we predict on those datasets for each fine tuned model and the general-purpose model.\n",
    "\n",
    "Predicting on the entire validation set takes some time.  OpenAI has a rate limit of 60 requests per minute.  It's also not free, so it's not something we want to have to do more than once.\n",
    "\n",
    "To make this code robust to things like network errors, we start by creating a dataframe that contains the validation data and a blank column for the results.  The code will run predictions for each row that has an empty result.  This means that this code can be restarted in case of failure.\n",
    "\n",
    "Additionally, to avoid having to re-run all the predictions in case of kernel restart, we save the resulting dataframe to file where it can be optionally reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared empty validation dataframe with 700 rows\n"
     ]
    }
   ],
   "source": [
    "# This cell prepares the dataframe that will hold the predictions\n",
    "\n",
    "rows = []\n",
    "for sample_size in fine_tuned_models.keys():\n",
    "    fineTunedModelId = fine_tuned_models[sample_size]\n",
    "    validation_data_path = f\"../data/temp/model_{sample_size}/validation.jsonl\"\n",
    "    with open(validation_data_path, 'r') as f:\n",
    "\n",
    "        # To test this on a smaller dataset, we can optionally use \"[:5]\" to take only the first 5 lines\n",
    "        # for line in f.readlines()[:5]:\n",
    "        for line in f.readlines():\n",
    "            data = json.loads(line)\n",
    "            prompt = data['messages'][1]['content']\n",
    "            completion = data['messages'][2]['content']\n",
    "            rows.append({\n",
    "                'model': fineTunedModelId,\n",
    "                'sample_size': sample_size,\n",
    "                'prompt': prompt,\n",
    "                'expected': completion == 'spam',\n",
    "                'predicted': \"-\"\n",
    "            })\n",
    "            rows.append({\n",
    "                'model': 'general',\n",
    "                'sample_size': sample_size,\n",
    "                'prompt': prompt,\n",
    "                'expected': completion == 'spam',\n",
    "                'predicted': \"-\"\n",
    "            })    \n",
    "\n",
    "validation_df = pd.DataFrame(rows)      \n",
    "print(\"Prepared empty validation dataframe with {} rows\".format(len(validation_df)))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded validation data, 645 items remaining\n"
     ]
    }
   ],
   "source": [
    "# If a previous result is available, we can optionally load it here instead of re-running the validation\n",
    "validation_df = pd.read_csv('../data/temp/validation_results.csv')\n",
    "print(\"Loaded validation data, {} items remaining\".format(validation_df['predicted'].eq(\"-\").sum()))    \n",
    "# validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation, 645 items remaining\n",
      "0:00:00.005244  Predicting on validation row 56 / 700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.608967  Predicting on validation row 57 / 700\n"
     ]
    }
   ],
   "source": [
    "# Note: Running this cell will take a while and incur API usage costs\n",
    "\n",
    "# Use Throttler to limit the number of requests per minute\n",
    "from throttler import Throttler\n",
    "throttler = Throttler(rate_limit=19, period=20)\n",
    "\n",
    "print(\"Running validation, {} items remaining\".format(validation_df['predicted'].eq(\"-\").sum()))     \n",
    "\n",
    "start = time.time()\n",
    "for index, row in validation_df.iterrows():\n",
    "    if row['predicted'] == \"-\":\n",
    "        async with throttler:\n",
    "            elapsedSeconds = time.time() - start\n",
    "            print(\"{}  Predicting on validation row {} / {}\".format(str(datetime.timedelta(seconds=elapsedSeconds)), index+1, len(validation_df)))\n",
    "            if row['model'] == 'general':\n",
    "                result = await getSpamClassification_GeneralModel(row['prompt'])\n",
    "            else:\n",
    "                result = await getSpamClassification_FineTune(row['model'], row['prompt'])\n",
    "        validation_df.loc[index, 'predicted'] = result\n",
    "\n",
    "        # Save to disk after every prediction in case we need to interrupt kernel and resume\n",
    "        validation_df.to_csv('../data/temp/validation_results.csv', index=False)\n",
    "\n",
    "\n",
    "validation_df['predicted'] = validation_df['predicted'].astype(bool)\n",
    "validation_df['correct'] = validation_df['expected'] == validation_df['predicted']\n",
    "validation_df.to_csv('../data/temp/validation_results.csv', index=False)\n",
    "\n",
    "print(\"Saved validation results to ../data/temp/validation_results.csv\")\n",
    "validation_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_size</th>\n",
       "      <th>model</th>\n",
       "      <th>true_positive</th>\n",
       "      <th>false_positive</th>\n",
       "      <th>false_negative</th>\n",
       "      <th>true_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>general</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>general</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>general</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_size       model  true_positive  false_positive  false_negative  \\\n",
       "0           50  fine-tuned              5               0               0   \n",
       "1           50     general              5               0               0   \n",
       "2          100  fine-tuned              5               0               0   \n",
       "3          100     general              5               0               0   \n",
       "4          200  fine-tuned              5               0               0   \n",
       "5          200     general              5               0               0   \n",
       "\n",
       "   true_negative  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "5              0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a confusion matrix for each sample size and model, put them into a dataframe\n",
    "\n",
    "#%pip install scikit-learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "grouped = validation_df.groupby('sample_size')\n",
    "\n",
    "rows = []\n",
    "for sample_size in grouped.groups:\n",
    "    group = grouped.get_group(sample_size)\n",
    "    # display(group)\n",
    "\n",
    "    fineTuneModelPredictions = group[group['model'] != 'general']\n",
    "    generalModelPredictions = group[group['model'] == 'general']\n",
    "\n",
    "    fineTuneConfusionMatrix = confusion_matrix(fineTuneModelPredictions['expected'], fineTuneModelPredictions['predicted'], labels=[True, False])\n",
    "    # print(fineTuneConfusionMatrix)\n",
    "    fineTuneModelAccuracy = (fineTuneConfusionMatrix[0][0] + fineTuneConfusionMatrix[1][1]) / (fineTuneConfusionMatrix[0][0] + fineTuneConfusionMatrix[0][1] + fineTuneConfusionMatrix[1][0] + fineTuneConfusionMatrix[1][1])\n",
    "    rows.append([sample_size, 'fine-tuned', fineTuneConfusionMatrix[0][0], fineTuneConfusionMatrix[0][1], fineTuneConfusionMatrix[1][0], fineTuneConfusionMatrix[1][1], fineTuneModelAccuracy])\n",
    "\n",
    "    generalConfusionMatrix = confusion_matrix(generalModelPredictions['expected'], generalModelPredictions['predicted'], labels=[True, False])\n",
    "    # print(generalConfusionMatrix)\n",
    "    generalModelAccuracy = (generalConfusionMatrix[0][0] + generalConfusionMatrix[1][1]) / (generalConfusionMatrix[0][0] + generalConfusionMatrix[0][1] + generalConfusionMatrix[1][0] + generalConfusionMatrix[1][1])\n",
    "    rows.append([sample_size, 'general', generalConfusionMatrix[0][0], generalConfusionMatrix[0][1], generalConfusionMatrix[1][0], generalConfusionMatrix[1][1], generalModelAccuracy])\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(rows, columns=['sample_size', 'model', 'true_positive', 'false_positive', 'false_negative', 'true_negative', 'accuracy'])\n",
    "confusion_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
