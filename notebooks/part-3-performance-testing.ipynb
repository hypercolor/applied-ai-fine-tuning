{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv; load_dotenv()\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_models = {\n",
    "    50: 'ft:gpt-3.5-turbo-1106:aa-engineering::8I9g9RO0',\n",
    "    100: 'ft:gpt-3.5-turbo-1106:aa-engineering::8I9vALSP',\n",
    "    200: 'ft:gpt-3.5-turbo-1106:aa-engineering::8IAIy8LD'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Classification APIs\n",
    "\n",
    "These APIs take an input message and use either a fine-tuned model or the general-purpose model to predict whether it is spam.  Each returns a boolean: True for spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install retry\n",
    "from retry import retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tuned Model API\n",
    "\n",
    "fineTunePrompt = \"You are a system for categorizing SMS text messages as being unwanted spam or normal messages.\"\n",
    "\n",
    "@retry(delay=0, backoff=2, max_delay=10)\n",
    "async def getSpamClassification_FineTune(fineTunedModelId, prompt):\n",
    "  completion = await openai.ChatCompletion.acreate(\n",
    "    model=fineTunedModelId,\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": fineTunePrompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "  )\n",
    "  result = completion.choices[0].message.content.lower() == 'spam'\n",
    "  # print(prompt, \"=>\", result)\n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Purpose Model API\n",
    "\n",
    "generalModelPrompt = \"You will be provided with a text message. You will need to classify the text message as spam, ham. Spam is a text message that is spam, harmful, abusive, or otherwise unwanted. Ham is a text message that is not spam.\"\n",
    "\n",
    "@retry(delay=0, backoff=2, max_delay=10)\n",
    "async def getSpamClassification_GeneralModel(message):\n",
    "    response = await openai.ChatCompletion.acreate(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": generalModelPrompt},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=256\n",
    "    )\n",
    "    return response.choices[0].message.content.lower() == 'spam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Validation Data\n",
    "\n",
    "Each fine-tuned model has a validation dataset in addition to its training data.  Here we predict on those datasets for each fine tuned model and the general-purpose model.\n",
    "\n",
    "Predicting on the entire validation set takes some time.  OpenAI has a rate limit of 60 requests per minute.  It's also not free, so it's not something we want to have to do more than once.\n",
    "\n",
    "To make this code robust to things like network errors, we start by creating a dataframe that contains the validation data and a blank column for the results.  The code will run predictions for each row that has an empty result.  This means that this code can be restarted in case of failure.\n",
    "\n",
    "Additionally, to avoid having to re-run all the predictions in case of kernel restart, we save the resulting dataframe to file where it can be optionally reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared empty validation dataframe with 700 rows\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for sample_size in fine_tuned_models.keys():\n",
    "    fineTunedModelId = fine_tuned_models[sample_size]\n",
    "    validation_data_path = f\"../data/temp/model_{sample_size}/validation.jsonl\"\n",
    "    with open(validation_data_path, 'r') as f:\n",
    "\n",
    "        # To test this on a smaller dataset, we can optionally use \"[:5]\" to take only the first 5 lines\n",
    "        # for line in f.readlines()[:5]:\n",
    "        for line in f.readlines():\n",
    "            data = json.loads(line)\n",
    "            prompt = data['messages'][1]['content']\n",
    "            completion = data['messages'][2]['content']\n",
    "            rows.append({\n",
    "                'model': fineTunedModelId,\n",
    "                'sample_size': sample_size,\n",
    "                'prompt': prompt,\n",
    "                'expected': completion == 'spam',\n",
    "                'predicted': None\n",
    "            })\n",
    "            rows.append({\n",
    "                'model': 'general',\n",
    "                'sample_size': sample_size,\n",
    "                'prompt': prompt,\n",
    "                'expected': completion == 'spam',\n",
    "                'predicted': None\n",
    "            })    \n",
    "\n",
    "validation_df = pd.DataFrame(rows)      \n",
    "print(\"Prepared empty validation dataframe with {} rows\".format(len(validation_df)))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a previous result is available, we can optionally load it here instead of re-running the validation\n",
    "# validation_df = pd.read_csv('../data/temp/validation_results.csv')\n",
    "# validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation, 700 items remaining\n",
      "0:00:00.000457  Predicting on validation row 1 / 700\n",
      "0:00:00.330594  Predicting on validation row 2 / 700\n",
      "0:00:00.737374  Predicting on validation row 3 / 700\n",
      "0:00:00.999987  Predicting on validation row 4 / 700\n",
      "0:00:01.416003  Predicting on validation row 5 / 700\n",
      "0:00:01.682397  Predicting on validation row 6 / 700\n",
      "0:00:02.041290  Predicting on validation row 7 / 700\n",
      "0:00:02.277774  Predicting on validation row 8 / 700\n",
      "0:00:02.972915  Predicting on validation row 9 / 700\n",
      "0:00:03.228665  Predicting on validation row 10 / 700\n",
      "0:00:03.807232  Predicting on validation row 11 / 700\n",
      "0:00:04.082794  Predicting on validation row 12 / 700\n",
      "0:00:04.635927  Predicting on validation row 13 / 700\n",
      "0:00:04.905513  Predicting on validation row 14 / 700\n",
      "0:00:05.549680  Predicting on validation row 15 / 700\n",
      "0:00:05.778098  Predicting on validation row 16 / 700\n",
      "0:00:06.516215  Predicting on validation row 17 / 700\n",
      "0:00:06.761079  Predicting on validation row 18 / 700\n",
      "0:00:07.356074  Predicting on validation row 19 / 700\n",
      "0:00:07.668818  Predicting on validation row 20 / 700\n",
      "0:00:08.266276  Predicting on validation row 21 / 700\n",
      "0:00:08.539617  Predicting on validation row 22 / 700\n",
      "0:00:09.130707  Predicting on validation row 23 / 700\n",
      "0:00:09.386695  Predicting on validation row 24 / 700\n",
      "0:00:09.960751  Predicting on validation row 25 / 700\n",
      "0:00:10.202342  Predicting on validation row 26 / 700\n",
      "0:00:10.771595  Predicting on validation row 27 / 700\n",
      "0:00:11.055683  Predicting on validation row 28 / 700\n",
      "0:00:11.428878  Predicting on validation row 29 / 700\n",
      "0:00:11.692332  Predicting on validation row 30 / 700\n",
      "0:00:12.343939  Predicting on validation row 31 / 700\n",
      "0:00:12.563225  Predicting on validation row 32 / 700\n",
      "0:00:13.813882  Predicting on validation row 33 / 700\n",
      "0:00:14.068730  Predicting on validation row 34 / 700\n",
      "0:00:14.721356  Predicting on validation row 35 / 700\n",
      "0:00:15.018916  Predicting on validation row 36 / 700\n",
      "0:00:15.361820  Predicting on validation row 37 / 700\n",
      "0:00:15.610873  Predicting on validation row 38 / 700\n",
      "0:00:16.172564  Predicting on validation row 39 / 700\n",
      "0:00:16.407560  Predicting on validation row 40 / 700\n",
      "0:00:17.102733  Predicting on validation row 41 / 700\n",
      "0:00:17.377014  Predicting on validation row 42 / 700\n",
      "0:00:17.693034  Predicting on validation row 43 / 700\n",
      "0:00:17.914997  Predicting on validation row 44 / 700\n",
      "0:00:18.251675  Predicting on validation row 45 / 700\n",
      "0:00:18.524279  Predicting on validation row 46 / 700\n",
      "0:00:19.067905  Predicting on validation row 47 / 700\n",
      "0:00:19.376446  Predicting on validation row 48 / 700\n",
      "0:00:19.727922  Predicting on validation row 49 / 700\n",
      "0:00:19.971264  Predicting on validation row 50 / 700\n",
      "0:00:20.409540  Predicting on validation row 51 / 700\n",
      "0:00:20.663740  Predicting on validation row 52 / 700\n",
      "0:00:21.237656  Predicting on validation row 53 / 700\n",
      "0:00:21.508380  Predicting on validation row 54 / 700\n",
      "0:00:22.070073  Predicting on validation row 55 / 700\n",
      "0:00:22.320085  Predicting on validation row 56 / 700\n",
      "0:00:22.701071  Predicting on validation row 57 / 700\n",
      "0:00:22.970690  Predicting on validation row 58 / 700\n",
      "0:00:23.510374  Predicting on validation row 59 / 700\n",
      "0:00:23.758098  Predicting on validation row 60 / 700\n",
      "0:01:00.002699  Predicting on validation row 61 / 700\n",
      "0:01:00.332257  Predicting on validation row 62 / 700\n",
      "0:01:00.893275  Predicting on validation row 63 / 700\n",
      "0:01:01.147823  Predicting on validation row 64 / 700\n",
      "0:01:01.735560  Predicting on validation row 65 / 700\n",
      "0:01:02.006546  Predicting on validation row 66 / 700\n",
      "0:01:02.577024  Predicting on validation row 67 / 700\n",
      "0:01:02.850495  Predicting on validation row 68 / 700\n",
      "0:01:03.426954  Predicting on validation row 69 / 700\n",
      "0:01:03.709660  Predicting on validation row 70 / 700\n",
      "0:01:04.415104  Predicting on validation row 71 / 700\n",
      "0:01:04.682217  Predicting on validation row 72 / 700\n",
      "0:01:05.075270  Predicting on validation row 73 / 700\n",
      "0:01:05.301101  Predicting on validation row 74 / 700\n"
     ]
    }
   ],
   "source": [
    "# Note: Running this cell will take a while and incur API usage costs\n",
    "\n",
    "# Use Throttler to limit the number of requests per minute\n",
    "# %pip install throttler\n",
    "from throttler import Throttler\n",
    "throttler = Throttler(rate_limit=58, period=60)\n",
    "\n",
    "# Use Semaphore to control the number of concurrent requests\n",
    "import asyncio\n",
    "semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "print(\"Running validation, {} items remaining\".format(validation_df['predicted'].isnull().sum()))\n",
    "\n",
    "# async def classifyRow(index, row):\n",
    "#     async with semaphore:\n",
    "#         async with throttler:\n",
    "#             print(\"Predicting on validation row {} / {}\".format(index+1, len(validation_df)))\n",
    "#             if row['model'] == 'general':\n",
    "#                 result = await getSpamClassification_GeneralModel(row['prompt'])\n",
    "#             else:\n",
    "#                 result = await getSpamClassification_FineTune(row['model'], row['prompt'])\n",
    "#         validation_df.loc[index, 'predicted'] = result\n",
    "\n",
    "# tasks = [classifyRow(index, row) for index, row in validation_df.iterrows()]\n",
    "\n",
    "# # Schedule the tasks for execution\n",
    "# for task in tasks:\n",
    "#     asyncio.ensure_future(task)\n",
    "\n",
    "# # Wait for all tasks to complete\n",
    "# await asyncio.gather(*tasks)\n",
    "\n",
    "# asyncio.run(asyncio.gather(*(classifyRow(index, row) for index, row in validation_df.iterrows())))        \n",
    "\n",
    "start = time.time()\n",
    "for index, row in validation_df.iterrows():\n",
    "    if row['predicted'] is None:\n",
    "        async with throttler:\n",
    "            elapsedSeconds = time.time() - start\n",
    "            print(\"{}  Predicting on validation row {} / {}\".format(str(datetime.timedelta(seconds=elapsedSeconds)), index+1, len(validation_df)))\n",
    "            if row['model'] == 'general':\n",
    "                result = await getSpamClassification_GeneralModel(row['prompt'])\n",
    "            else:\n",
    "                result = await getSpamClassification_FineTune(row['model'], row['prompt'])\n",
    "        validation_df.loc[index, 'predicted'] = result\n",
    "\n",
    "validation_df['predicted'] = validation_df['predicted'].astype(bool)\n",
    "validation_df['correct'] = validation_df['expected'] == validation_df['predicted']\n",
    "validation_df.to_csv('../data/temp/validation_results.csv', index=False)\n",
    "print(\"Saved validation results to ../data/temp/validation_results.csv\")\n",
    "validation_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_size</th>\n",
       "      <th>model</th>\n",
       "      <th>true_positive</th>\n",
       "      <th>false_positive</th>\n",
       "      <th>false_negative</th>\n",
       "      <th>true_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>general</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>general</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>general</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_size       model  true_positive  false_positive  false_negative  \\\n",
       "0           50  fine-tuned              5               0               0   \n",
       "1           50     general              5               0               0   \n",
       "2          100  fine-tuned              5               0               0   \n",
       "3          100     general              5               0               0   \n",
       "4          200  fine-tuned              5               0               0   \n",
       "5          200     general              5               0               0   \n",
       "\n",
       "   true_negative  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "5              0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a confusion matrix for each sample size and model, put them into a dataframe\n",
    "\n",
    "#%pip install scikit-learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "grouped = validation_df.groupby('sample_size')\n",
    "\n",
    "rows = []\n",
    "for sample_size in grouped.groups:\n",
    "    group = grouped.get_group(sample_size)\n",
    "    # display(group)\n",
    "\n",
    "    fineTuneModelPredictions = group[group['model'] != 'general']\n",
    "    generalModelPredictions = group[group['model'] == 'general']\n",
    "\n",
    "    fineTuneConfusionMatrix = confusion_matrix(fineTuneModelPredictions['expected'], fineTuneModelPredictions['predicted'], labels=[True, False])\n",
    "    # print(fineTuneConfusionMatrix)\n",
    "    fineTuneModelAccuracy = (fineTuneConfusionMatrix[0][0] + fineTuneConfusionMatrix[1][1]) / (fineTuneConfusionMatrix[0][0] + fineTuneConfusionMatrix[0][1] + fineTuneConfusionMatrix[1][0] + fineTuneConfusionMatrix[1][1])\n",
    "    rows.append([sample_size, 'fine-tuned', fineTuneConfusionMatrix[0][0], fineTuneConfusionMatrix[0][1], fineTuneConfusionMatrix[1][0], fineTuneConfusionMatrix[1][1], fineTuneModelAccuracy])\n",
    "\n",
    "    generalConfusionMatrix = confusion_matrix(generalModelPredictions['expected'], generalModelPredictions['predicted'], labels=[True, False])\n",
    "    # print(generalConfusionMatrix)\n",
    "    generalModelAccuracy = (generalConfusionMatrix[0][0] + generalConfusionMatrix[1][1]) / (generalConfusionMatrix[0][0] + generalConfusionMatrix[0][1] + generalConfusionMatrix[1][0] + generalConfusionMatrix[1][1])\n",
    "    rows.append([sample_size, 'general', generalConfusionMatrix[0][0], generalConfusionMatrix[0][1], generalConfusionMatrix[1][0], generalConfusionMatrix[1][1], generalModelAccuracy])\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(rows, columns=['sample_size', 'model', 'true_positive', 'false_positive', 'false_negative', 'true_negative', 'accuracy'])\n",
    "confusion_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
